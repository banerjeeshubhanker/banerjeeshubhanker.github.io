---
title: 'Statistical Learning Theory: Part 1'
date: 2022-08-03
permalink: /posts/2012/08/blog-post-1/
tags:
  - Term
  - Automatic Term Extraction
  - Language Technology
---




# Statistical Learning Theory: Part 1

<p align="justify">
Machine learning is fundamentally about making predictions or decisions based on data. For example, in regression problems, machine learning can be used to predict continuous outcomes such as housing prices based on features like location, size, and number of rooms. In classification problems, it can be used to make decisions, such as identifying whether an email is spam or not based on its content. The learning problem involves finding a function that performs well on unseen data by minimizing an appropriate notion of risk or error. In this post, we will delve into the formal formulation of the learning problem, specifically focusing on regression and classification, defining the true risk, and exploring the optimal solutions: the Bayes optimal regressor and Bayes optimal classifier.
</p>

## 1. Formulating the Learning Problem

<p align="justify">
The goal of supervised learning is to learn a function $f$ that maps inputs $X$ to outputs $Y$. This function is chosen from a hypothesis class $\mathcal{H}$. The data comes from an unknown distribution $P(X, Y)$, and the task is to find $f$ that minimizes the expected loss over this distribution.
</p>

## 2. Defining and Explaining True Risk

<p align="justify">
The true risk (or expected risk) measures how well a function $f$ performs on unseen data. It is defined as the expected value of the loss function with respect to the joint distribution $P(X, Y)$.
</p>

**Mathematically, the true risk is given by:**

$$ 
\mathcal{R}(f) = \mathbb{E}_{(X, Y) \sim P} \left[ L(Y, f(X)) \right] = \int_{\mathcal{X} \times \mathcal{Y}} L(y, f(x)) \, dP(x, y)
$$

<p align="justify">
- For regression, this typically involves minimizing the squared error between the predicted and actual values.
- For classification, it involves minimizing the probability of incorrect classification.

The true risk represents the average error over the entire data distribution, making it the ultimate measure of a modelâ€™s performance. However, because $P(X, Y)$ is unknown, the true risk is typically approximated using empirical data.
</p>

## 3. Bayes Optimal Regressor

<p align="justify">
The Bayes optimal regressor is the function that minimizes the true risk under the Mean Squared Error loss. It represents the theoretically best predictor of $Y$ given $X$.
</p>

**Definition and Derivation:**

To find the Bayes optimal regressor, we minimize the true risk under the Mean Squared Error (MSE) loss. Here's a step-by-step derivation:

1. **Expand the Squared Loss:**

   Start by expanding the term inside the expectation:

   $$
     (Y - f(X))^2 = Y^2 - 2Yf(X) + f(X)^2.
   $$

2. **Conditional Expectation with Respect to $Y$ Given $X$:**

   For a fixed $X = x$, the expectation of the loss with respect to $Y$ is:

   $$
   \mathbb{E}[(Y - f(X))^2 \mid X = x] = \mathbb{E}[Y^2 \mid X = x] - 2f(x)\mathbb{E}[Y \mid X = x] + f(x)^2.
   $$

   Denote $\mathbb{E}[Y \mid X = x]$ as $\mu(x)$ for simplicity:

   $$
   \mathbb{E}[(Y - f(X))^2 \mid X = x] = \mathbb{E}[Y^2 \mid X = x] - 2f(x)\mu(x) + f(x)^2.
   $$

3. **Simplify Using the Variance Formula:**

   Recall the variance formula:

   $$
   \text{Var}(Y \mid X = x) = \mathbb{E}[Y^2 \mid X = x] - (\mathbb{E}[Y \mid X = x])^2 = \mathbb{E}[Y^2 \mid X = x] - \mu(x)^2.
   $$

   Substitute this back into the expectation:

   $$
   \mathbb{E}[Y^2 \mid X = x] = \text{Var}(Y \mid X = x) + \mu(x)^2.
   $$

   Therefore, the expectation becomes:

   $$
   \mathbb{E}[(Y - f(X))^2 \mid X = x] = \text{Var}(Y \mid X = x) + \mu(x)^2 - 2f(x)\mu(x) + f(x)^2.
   $$

4. **Minimize with Respect to $f(x)$:**

   To find the optimal function $f(x)$, differentiate the above expression with respect to $f(x)$ and set it to zero:

   $$
   \frac{\partial}{\partial f(x)} \left[ f(x)^2 - 2f(x)\mu(x) + \mu(x)^2 + \text{Var}(Y \mid X = x) \right] = 2f(x) - 2\mu(x).
   $$

   Setting this derivative to zero:

   $$
   2f(x) - 2\mu(x) = 0.
   $$

   Simplifying, we get:

   $$
   f(x) = \mu(x) = \mathbb{E}[Y \mid X = x].
   $$

<p align="justify">
which is the conditional mean of $Y$ given $X$. This is known as the Bayes optimal regressor because it minimizes the expected squared error.
</p>

## 4. Bayes Optimal Classifier

<p align="justify">
The Bayes optimal classifier minimizes the true risk under the 0-1 loss. It assigns each input $X$ to the class with the highest posterior probability given $X$.
</p>

**Definition and Derivation:**

The true risk for classification is:

$$
\mathcal{R}(f) = \mathbb{E}_{(X, Y) \sim P} \left[ \mathbb{I}(Y \neq f(X)) \right].
$$

For a fixed input $X = x$, the conditional risk of assigning $X = x$ to class $k$ is:

$$
\mathcal{R}(k \mid X = x) = 1 - P(Y = k \mid X = x).
$$

Minimizing this conditional risk involves choosing the class $k$ that maximizes the posterior probability:

$$
f^*(X) = \arg\max_{k \in \mathcal{Y}} P(Y = k \mid X).
$$

<p align="justify">
This classifier is optimal because it directly minimizes the expected misclassification error, making it the best possible decision rule under the 0-1 loss.
</p>

## 5. The Challenge of Unknown Distributions

<p align="justify">
One of the key challenges in machine learning is that the true distribution $P(X, Y)$ is unknown, making direct minimization of the true risk impractical. Instead, we rely on empirical risk minimization as an approximation based on observed data. This approach involves minimizing the loss on a finite sample of data, which serves as a proxy for the true risk, allowing us to approximate the Bayes optimal solutions as closely as possible.
</p>

## 6. Conclusion

<p align="justify">
In this post, we formulated the learning problem, defined regression and classification tasks, explained the concept of true risk, and derived the Bayes optimal regressor and classifier. Understanding these fundamental concepts highlights the theoretical limits of predictive modeling and underscores the goal of statistical learning: to approximate these optimal solutions as closely as possible using real-world data.
</p>
------
